{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Conformal Prediction Analysis\n",
    "\n",
    "This notebook analyzes protein similarity search results using conformal prediction methods.\n",
    "\n",
    "## Background\n",
    "- **Protein embeddings**: Vector representations of proteins that capture their structural/functional properties\n",
    "- **Similarity scores (S_i)**: How similar retrieved proteins are to the query protein (higher = more similar)\n",
    "- **FDR (False Discovery Rate)**: Proportion of incorrect matches among all retrieved matches\n",
    "- **FNR (False Negative Rate)**: Proportion of correct matches that were missed\n",
    "- **Lambda (λ)**: Similarity threshold - only retrieve proteins with similarity ≥ λ\n",
    "- **Conformal prediction**: Statistical method to provide prediction sets with guaranteed coverage\n",
    "\n",
    "The goal is to control error rates while retrieving relevant proteins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data\n",
    "\n",
    "Setting up the analysis environment and loading protein search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import binom, norm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from protein_conformal.util import *\n",
    "# from util import simplifed_venn_abers_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load protein search results**: Each entry contains similarity scores (S_i) and ground truth labels for protein matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Score Distribution Analysis\n",
    "\n",
    "Examining the distribution of similarity scores to understand the data characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "filename = 'pfam_new_proteins.npy'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    data_path = filename\n",
    "elif os.path.exists(os.path.join('..', 'data', filename)):\n",
    "    data_path = os.path.join('..', 'data', filename)\n",
    "elif os.path.exists(os.path.join('data', filename)):\n",
    "    data_path = os.path.join('data', filename)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cannot find {filename}\")\n",
    "\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "print(len(data))\n",
    "print(len(data[0]['S_i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# filename = 'pfam_new_proteins.npy'\n",
    "\n",
    "# # Test all possible paths\n",
    "# test_paths = [\n",
    "#     filename,\n",
    "#     os.path.join('..', 'data', filename),\n",
    "#     os.path.join('data', filename)\n",
    "# ]\n",
    "\n",
    "# print(\"Testing file existence:\")\n",
    "# for i, path in enumerate(test_paths):\n",
    "#     exists = os.path.exists(path)\n",
    "#     if exists:\n",
    "#         abs_path = os.path.abspath(path)\n",
    "#         print(f\"Path {i+1}: {path} -> EXISTS at {abs_path}\")\n",
    "#     else:\n",
    "#         print(f\"Path {i+1}: {path} -> NOT FOUND\")\n",
    "\n",
    "# print(f\"\\nCurrent working directory: {os.getcwd()}\")\n",
    "\n",
    "# # Load and check the data\n",
    "# try:\n",
    "#     # Use your existing logic\n",
    "#     if os.path.exists(filename):\n",
    "#         data_path = filename\n",
    "#     elif os.path.exists(os.path.join('..', 'data', filename)):\n",
    "#         data_path = os.path.join('..', 'data', filename)\n",
    "#     elif os.path.exists(os.path.join('data', filename)):\n",
    "#         data_path = os.path.join('data', filename)\n",
    "#     else:\n",
    "#         raise FileNotFoundError(f\"Cannot find {filename}\")\n",
    "    \n",
    "#     print(f\"\\nUsing path: {data_path}\")\n",
    "#     print(f\"Absolute path: {os.path.abspath(data_path)}\")\n",
    "    \n",
    "#     data = np.load(data_path, allow_pickle=True)\n",
    "#     print(f\"\\nData loaded successfully:\")\n",
    "#     print(f\"Number of entries: {len(data)}\")\n",
    "#     print(f\"Type of data[0]: {type(data[0])}\")\n",
    "#     print(f\"Keys in data[0]: {list(data[0].keys()) if hasattr(data[0], 'keys') else 'Not a dict'}\")\n",
    "    \n",
    "#     if hasattr(data[0], '__getitem__') and 'S_i' in data[0]:\n",
    "#         print(f\"Length of data[0]['S_i']: {len(data[0]['S_i'])}\")\n",
    "#     else:\n",
    "#         print(\"data[0]['S_i'] not accessible\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at distribution of $S_{ij}$ across retrieved hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformal prediction assumes data points are exchangeable (can be reordered without changing joint distribution).\n",
    "\n",
    "We split data by time (before/after cutoff) to test if distributions changed over time, which would violate exchangeability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Protein-vec: Histogram of Similarity Scores')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = np.stack([query['S_i'] for query in data], axis=0)\n",
    "plt.hist(sims.flatten(), bins=200)\n",
    "plt.xlabel('Embedding Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Protein-vec: Histogram of Similarity Scores')\n",
    "# plt.show()\n",
    "# plt.savefig('/data/ron/protein-conformal/data/pvec_histogram.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99984348 0.99986696 0.9998917  0.99995911]\n"
     ]
    }
   ],
   "source": [
    "# get the 20th, 40th, 60th, 80th percentiles of the similarity scores\n",
    "percentiles = [20, 50, 80, 99]\n",
    "percentile_values = np.percentile(sims.flatten(), percentiles)\n",
    "print(percentile_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for exchangability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For a given threshold λ, calculate FDR = (false discoveries) / (total discoveries)\n",
    "\n",
    "This measures the proportion of retrieved proteins that are actually incorrect matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870,)\n"
     ]
    }
   ],
   "source": [
    "filename = 'new_proteins_after_cutoff.npy'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    data_path = filename\n",
    "elif os.path.exists(os.path.join('..', 'data', filename)):\n",
    "    data_path = os.path.join('..', 'data', filename)\n",
    "elif os.path.exists(os.path.join('data', filename)):\n",
    "    data_path = os.path.join('data', filename)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Cannot find {filename}\")\n",
    "\n",
    "protein_date_cutoff = np.load(data_path, allow_pickle=True)\n",
    "print(data[protein_date_cutoff].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870, 10000)\n",
      "(870, 10000)\n"
     ]
    }
   ],
   "source": [
    "# calculate risk for every datapoint above a certain threshold lambda\n",
    "labels = np.stack([query['exact'] for query in data[protein_date_cutoff]], axis=0)\n",
    "sims = np.stack([query['S_i'] for query in data[protein_date_cutoff]], axis=0)\n",
    "print(labels.shape)\n",
    "print(sims.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(994, 10000)\n",
      "(994, 10000)\n"
     ]
    }
   ],
   "source": [
    "labels_after = np.stack([query['exact'] for query in data[~protein_date_cutoff]], axis=0)\n",
    "sims_after = np.stack([query['S_i'] for query in data[~protein_date_cutoff]], axis=0)\n",
    "print(labels_after.shape)\n",
    "print(sims_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_1d(sims, labels, lam):\n",
    "    # FDR: Number of false matches / number of matches\n",
    "    total_discoveries = (sims >= lam).sum()\n",
    "    false_discoveries = ((1 - labels) * (sims >= lam)).sum()\n",
    "    total_discoveries = np.maximum(total_discoveries, 1)\n",
    "    fdr = false_discoveries / total_discoveries\n",
    "    return fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.9999520198675599\n",
    "# calculate risk for every datapoint\n",
    "risks_before = [risk_1d(sims[i], labels[i], lam) for i in range(len(labels))]\n",
    "risks_after = [risk_1d(sims_after[i], labels_after[i], lam) for i in range(len(labels_after))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram for risks_before\n",
    "counts_before, bin_edges_before = np.histogram(risks_before, bins=100, density=True)\n",
    "cdf_before = np.cumsum(counts_before)  # Cumulative sum\n",
    "cdf_before = cdf_before / cdf_before[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "# Compute histogram for risks_after\n",
    "counts_after, bin_edges_after = np.histogram(risks_after, bins=100, density=True)\n",
    "cdf_after = np.cumsum(counts_after)  # Cumulative sum\n",
    "cdf_after = cdf_after / cdf_after[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "# Plot CDFs\n",
    "plt.plot(bin_edges_before[1:], cdf_before, label='Before', alpha=0.5)\n",
    "plt.plot(bin_edges_after[1:], cdf_after, label='After', alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Risk')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend()\n",
    "plt.title(rf'CDF of Risk at $\\lambda$ = {lam:.5f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many correct matches we miss at different thresholds\n",
    "\n",
    "FNR is computed by looking at proteins that should match but have similarity < λ (so they get rejected).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten the 2x2 array for easier indexing\n",
    "\n",
    "for idx, lam in enumerate(percentile_values):\n",
    "    risks_before = [risk_1d(sims[i], labels[i], lam) for i in range(len(labels))]\n",
    "    risks_after = [risk_1d(sims_after[i], labels_after[i], lam) for i in range(len(labels_after))]\n",
    "\n",
    "    # Compute histogram for risks_before\n",
    "    counts_before, bin_edges_before = np.histogram(risks_before, bins=100, density=True)\n",
    "    cdf_before = np.cumsum(counts_before)  # Cumulative sum\n",
    "    cdf_before = cdf_before / cdf_before[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "    # Compute histogram for risks_after\n",
    "    counts_after, bin_edges_after = np.histogram(risks_after, bins=100, density=True)\n",
    "    cdf_after = np.cumsum(counts_after)  # Cumulative sum\n",
    "    cdf_after = cdf_after / cdf_after[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "    # Plot CDFs\n",
    "    ax = axes[idx]\n",
    "    ax.plot(bin_edges_before[1:], cdf_before, label='2022-05-25 to 2022-12-14', alpha=0.5)\n",
    "    ax.plot(bin_edges_after[1:], cdf_after, label='2022-12-14 to 2023-06-28', alpha=0.5)\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(rf'$\\lambda$ = {lam:.5f}')\n",
    "    \n",
    "    # Remove inner labels to declutter the plots\n",
    "    ax.label_outer()\n",
    "\n",
    "    # Add legend to the top-right subplot (index 1)\n",
    "    if idx == 1:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(0.9, 1))  # Shift legend to the left\n",
    "\n",
    "# Add labels and legend\n",
    "# axes[-1].set_xlabel('Risk', fontsize=12)  # Set x-label on the last subplot\n",
    "# axes[0].set_ylabel('CDF', fontsize=12)    # Set y-label on the first subplot\n",
    "\n",
    "fig.text(0.02, 0.5, 'CDF', va='center', rotation='vertical', fontsize=16)\n",
    "fig.text(0.5, 0.02, 'Risk', ha='center', fontsize=16)  # Larger label for Risk\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "# handles, labels_fig = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels_fig, loc='', ncol=2, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# Adjust layout to make space for the new labels and ensure plots fit well\n",
    "fig.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n",
    "\n",
    "# Show the plot (uncomment in a script or notebook)\n",
    "# plt.show()\n",
    "# plt.savefig('/data/risk_cdf.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()  # Flatten the 2x2 array for easier indexing\n",
    "\n",
    "for idx, lam in enumerate(percentile_values):\n",
    "    risks_before = [calculate_false_negatives(sims[i], labels[i], lam) for i in range(len(labels))]\n",
    "    risks_after = [calculate_false_negatives(sims_after[i], labels_after[i], lam) for i in range(len(labels_after))]\n",
    "\n",
    "    # Compute histogram for risks_before\n",
    "    counts_before, bin_edges_before = np.histogram(risks_before, bins=100, density=True)\n",
    "    cdf_before = np.cumsum(counts_before)  # Cumulative sum\n",
    "    cdf_before = cdf_before / cdf_before[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "    # Compute histogram for risks_after\n",
    "    counts_after, bin_edges_after = np.histogram(risks_after, bins=100, density=True)\n",
    "    cdf_after = np.cumsum(counts_after)  # Cumulative sum\n",
    "    cdf_after = cdf_after / cdf_after[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "    # Plot CDFs\n",
    "    ax = axes[idx]\n",
    "    ax.plot(bin_edges_before[1:], cdf_before, label='2022-05-25 to 2022-12-14', alpha=0.5)\n",
    "    ax.plot(bin_edges_after[1:], cdf_after, label='2022-12-14 to 2023-06-28', alpha=0.5)\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(rf'$\\lambda$ = {lam:.5f}')\n",
    "    \n",
    "    # Add legend to the top-right subplot (index 1)\n",
    "    if idx == 1:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(0.9, .7))  # Shift legend to the left\n",
    "\n",
    "    # Remove inner labels to declutter the plots\n",
    "    ax.label_outer()\n",
    "\n",
    "# Add labels and legend\n",
    "# axes[-1].set_xlabel('Risk', fontsize=12)  # Set x-label on the last subplot\n",
    "# axes[0].set_ylabel('CDF', fontsize=12)    # Set y-label on the first subplot\n",
    "\n",
    "fig.text(0.02, 0.5, 'CDF', va='center', rotation='vertical', fontsize=16)\n",
    "fig.text(0.5, 0.02, 'FNR', ha='center', fontsize=16)  # Larger label for Risk\n",
    "\n",
    "# Add a single legend for all subplots\n",
    "# handles, labels_fig = ax.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels_fig, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# Adjust layout to make space for the new labels and ensure plots fit well\n",
    "fig.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n",
    "\n",
    "# Show the plot (uncomment in a script or notebook)\n",
    "# plt.show()\n",
    "# plt.savefig(source_data_path + '/data/fnr_cdf.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([24.74849095,  1.20724346,  0.80482897,  1.20724346,  0.80482897,\n",
       "         1.00603622,  0.40241449,  0.50301811,  0.70422535,  0.30181087,\n",
       "         0.50301811,  1.30784708,  0.60362173,  0.80482897,  0.70422535,\n",
       "         0.70422535,  0.80482897,  0.80482897,  0.10060362,  0.30181087,\n",
       "         0.70422535,  0.40241449,  0.70422535,  0.60362173,  0.30181087,\n",
       "         0.60362173,  0.70422535,  0.80482897,  0.30181087,  0.20120724,\n",
       "         1.81086519,  0.40241449,  0.20120724,  0.80482897,  0.40241449,\n",
       "         0.50301811,  0.50301811,  0.50301811,  0.70422535,  0.50301811,\n",
       "         0.60362173,  0.20120724,  0.50301811,  0.30181087,  0.60362173,\n",
       "         0.30181087,  0.30181087,  0.50301811,  0.50301811,  0.30181087,\n",
       "         2.41448692,  0.50301811,  1.00603622,  0.50301811,  0.9054326 ,\n",
       "         0.40241449,  0.40241449,  0.70422535,  0.30181087,  0.20120724,\n",
       "         0.80482897,  0.20120724,  0.80482897,  1.20724346,  1.20724346,\n",
       "         0.30181087,  0.9054326 ,  0.20120724,  0.60362173,  0.50301811,\n",
       "         1.10663984,  0.50301811,  1.4084507 ,  1.10663984,  0.60362173,\n",
       "         1.30784708,  0.40241449,  1.10663984,  0.9054326 ,  0.20120724,\n",
       "         0.70422535,  0.80482897,  1.00603622,  0.40241449,  0.9054326 ,\n",
       "         1.00603622,  0.30181087,  0.40241449,  1.20724346,  0.70422535,\n",
       "         1.50905433,  0.60362173,  0.70422535,  1.50905433,  1.30784708,\n",
       "         0.60362173,  1.10663984,  1.10663984,  0.70422535,  6.53923541]),\n",
       " array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "        0.99, 1.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize by density\n",
    "# weights = np.ones_like(risks_before)/float(len(risks_before))\n",
    "plt.hist(risks_before, \n",
    "         bins=100,\n",
    "        #  weights=weights, \n",
    "         alpha=0.5, \n",
    "         label='Before', \n",
    "         density=True)\n",
    "\n",
    "# weights = np.ones_like(risks_after)/float(len(risks_after))\n",
    "plt.hist(risks_after,\n",
    "            bins=100,\n",
    "            # weights=weights, \n",
    "            alpha=0.5, \n",
    "            label='After',\n",
    "            density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n",
      "No actual positives\n"
     ]
    }
   ],
   "source": [
    "# testing FNR for the same threshold\n",
    "# fnr = [calculate_false_negatives(sims, labels, lam) for lam in lambdas]\n",
    "lam = 0.999920198675599\n",
    "fnr_before = [calculate_false_negatives(sims[i], labels[i], lam) for i in range(len(labels))]\n",
    "fnr_after = [calculate_false_negatives(sims_after[i], labels_after[i], lam) for i in range(len(labels_after))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([55.43259557,  5.03018109,  3.722334  ,  2.71629779,  2.61569416,\n",
       "         1.60965795,  1.00603622,  1.30784708,  1.60965795,  1.30784708,\n",
       "         0.50301811,  0.60362173,  0.60362173,  0.10060362,  0.60362173,\n",
       "         1.00603622,  0.80482897,  0.50301811,  0.9054326 ,  0.9054326 ,\n",
       "         0.70422535,  0.50301811,  0.40241449,  0.50301811,  0.20120724,\n",
       "         0.80482897,  0.60362173,  0.60362173,  0.20120724,  0.20120724,\n",
       "         0.70422535,  0.20120724,  0.50301811,  0.50301811,  0.70422535,\n",
       "         0.10060362,  0.60362173,  0.10060362,  0.40241449,  0.40241449,\n",
       "         0.        ,  0.20120724,  0.20120724,  0.10060362,  0.30181087,\n",
       "         0.10060362,  0.10060362,  0.30181087,  0.10060362,  0.        ,\n",
       "         0.50301811,  0.30181087,  0.        ,  0.10060362,  0.20120724,\n",
       "         0.20120724,  0.10060362,  0.20120724,  0.20120724,  0.20120724,\n",
       "         0.        ,  0.        ,  0.10060362,  0.10060362,  0.30181087,\n",
       "         0.        ,  0.30181087,  0.        ,  0.10060362,  0.20120724,\n",
       "         0.10060362,  0.        ,  0.        ,  0.20120724,  0.        ,\n",
       "         0.40241449,  0.        ,  0.        ,  0.20120724,  0.10060362,\n",
       "         0.30181087,  0.10060362,  0.        ,  0.        ,  0.30181087,\n",
       "         0.10060362,  0.10060362,  0.20120724,  0.        ,  0.20120724,\n",
       "         0.        ,  0.        ,  0.10060362,  0.10060362,  0.10060362,\n",
       "         0.        ,  0.10060362,  0.10060362,  0.20120724,  0.60362173]),\n",
       " array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "        0.99, 1.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights = np.ones_like(risks_before)/float(len(risks_before))\n",
    "plt.hist(fnr_before, \n",
    "         bins=100,\n",
    "        #  weights=weights, \n",
    "         alpha=0.5, \n",
    "         label='Before', \n",
    "         density=True)\n",
    "\n",
    "# weights = np.ones_like(risks_after)/float(len(risks_after))\n",
    "plt.hist(fnr_after,\n",
    "            bins=100,\n",
    "            # weights=weights, \n",
    "            alpha=0.5, \n",
    "            label='After',\n",
    "            density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute histogram for risks_before\n",
    "counts_before, bin_edges_before = np.histogram(fnr_before, bins=100, density=True)\n",
    "cdf_before = np.cumsum(counts_before)  # Cumulative sum\n",
    "cdf_before = cdf_before / cdf_before[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "# Compute histogram for risks_after\n",
    "counts_after, bin_edges_after = np.histogram(fnr_after, bins=100, density=True)\n",
    "cdf_after = np.cumsum(counts_after)  # Cumulative sum\n",
    "cdf_after = cdf_after / cdf_after[-1]  # Normalize to turn it into a proper CDF\n",
    "\n",
    "# Plot CDFs\n",
    "plt.plot(bin_edges_before[1:], cdf_before, label='Before', alpha=0.5)\n",
    "plt.plot(bin_edges_after[1:], cdf_after, label='After', alpha=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('FNR')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend()\n",
    "plt.title('CDF of FNR Before and After')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnr_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "994"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnr_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.03599528203704989, pvalue=0.5680407193538268, statistic_location=0.0, statistic_sign=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the hypothesis that the two samples come from the same distribution\n",
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(risks_before, risks_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR for exact hits on Pfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sims, labels = get_sims_labels(data, partial=False)\n",
    "labels = np.stack([query['exact'] for query in data], axis=0)\n",
    "sims = np.stack([query['S_i'] for query in data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1864, 10000), (1864, 10000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape, sims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1508, 1454, 1385, ...,    1,    0,    0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(sims.min(),sims.max(),100)\n",
    "risks = [risk(sims, labels, lam) for lam in lambdas]\n",
    "risks_no_empties = [risk_no_empties(sims, labels, lam) for lam in lambdas]\n",
    "# no_empties: as we increase lambda, the number of empty sets increases. We filter out the empty sets and calculate the risk.\n",
    "fnr = [calculate_false_negatives(sims, labels, lam) for lam in lambdas]\n",
    "# percentage_of_discoveries_rsk = [percentage_of_discoveries(sims, labels, lam) for lam in lambdas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999919675488975"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FDR + FNR as a function of lambda\n",
    "plt.figure(figsize=(3.5, 2.5))\n",
    "plt.plot(lambdas, risks, label='Marginal risk')\n",
    "# plt.plot(lambdas, risks_no_empties, label='Risk (no empty sets)')\n",
    "# plt.plot(lambdas, percentage_of_discoveries_rsk, label='Percentage of discoveries')\n",
    "plt.plot(lambdas, fnr, label='False negative rate')\n",
    "# Customize the plot\n",
    "plt.xlabel(r'Threshold $\\lambda$', fontsize=10)\n",
    "plt.ylabel('Error rate', fontsize=10)\n",
    "plt.title(r'Error rates across $\\lambda$ thresholds', fontsize=14)\n",
    "\n",
    "# Adjust x-ticks to make the x-axis less crowded\n",
    "plt.xticks(np.linspace(np.min(lambdas), np.max(lambdas), 3), [f'{x:.5f}' for x in np.linspace(np.min(lambdas), np.max(lambdas), 3)], fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend()\n",
    "# Customize grid and remove top and right spines\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "sns.despine()\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/pvec_error_rates.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR / FNR on partial hits on Pfam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FDR as a function of lambda\n",
    "sims, labels_partial = get_sims_labels(data, partial=True)\n",
    "# labels_partial = np.stack([(np.sum(query['partial'], axis=1) >= 1) if len(np.array(query['partial']).shape) > 1 else query['partial'] for query in data], axis=0)\n",
    "# sims = np.stack([query['S_i'] for query in data], axis=0)\n",
    "lambdas = np.linspace(sims.min(),sims.max(),100)\n",
    "risks = [risk(sims, labels_partial, lam) for lam in lambdas]\n",
    "risks_no_empties = [risk_no_empties(sims, labels_partial, lam) for lam in lambdas]\n",
    "# no_empties: as we increase lambda, the number of empty sets increases. We filter out the empty sets and calculate the risk.\n",
    "fnr = [calculate_false_negatives(sims, labels_partial, lam) for lam in lambdas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FDR + FNR as a function of lambda\n",
    "plt.figure(figsize=(3.5, 2.5))\n",
    "plt.plot(lambdas, risks, label='Marginal risk')\n",
    "# plt.plot(lambdas, risks_no_empties, label='Risk (no empty sets)')\n",
    "# plt.plot(lambdas, percentage_of_discoveries_rsk, label='Percentage of discoveries')\n",
    "plt.plot(lambdas, fnr, label='False negative rate')\n",
    "plt.xlabel(r'Threshold $\\lambda$', fontsize=10)\n",
    "plt.ylabel('Error rate', fontsize=10)\n",
    "plt.title(r'Error rates across $\\lambda$ thresholds (partial match)', fontsize=14)\n",
    "# Adjust x-ticks to make the x-axis less crowded\n",
    "plt.xticks(np.linspace(np.min(lambdas), np.max(lambdas), 5), [f'{x:.5f}' for x in np.linspace(np.min(lambdas), np.max(lambdas), 5)], fontsize=10, rotation=45)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "sns.despine()\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/pvec_error_rates_partial.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn then test for FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0.1, lhat=0.9999806327049179, risk=0.08183884226851114\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "lhat, risk_0 = get_thresh_FDR(labels, sims, alpha, delta=0.5, N=100)\n",
    "print(f'alpha={alpha}, lhat={lhat}, risk={risk_0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:11<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "risks = []\n",
    "tprs = []\n",
    "lhats = []\n",
    "fdr_cals = []\n",
    "alpha = 0.1\n",
    "num_trials = 100\n",
    "n_calib = 100\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    np.random.shuffle(data)\n",
    "    cal_data = data[:n_calib]\n",
    "    test_data = data[n_calib:2*n_calib]\n",
    "    X_cal, y_cal = get_sims_labels(cal_data, partial=False)\n",
    "    X_test, y_test_exact = get_sims_labels(test_data, partial=False)\n",
    "    # sims, labels = get_sims_labels(cal_data, partial=False)\n",
    "    lhat, fdr_cal = get_thresh_FDR(y_cal, X_cal, alpha, delta=0.5, N=100)\n",
    "    fdr_cals.append(fdr_cal)\n",
    "    lhats.append(lhat)\n",
    "    # print(X_test.shape)\n",
    "    # print(y_test_exact.shape)\n",
    "    risks.append(risk(X_test, y_test_exact, lhat))\n",
    "    tprs.append(calculate_true_positives(X_test, y_test_exact, lhat))\n",
    "    # print(lhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'FPR at $\\\\alpha=0.1$ FDR')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "weights = np.ones_like(risks)/float(len(risks))\n",
    "plt.hist(np.array(tprs), weights=weights, color='skyblue')\n",
    "plt.xlabel('True positive rate', fontsize=10)\n",
    "plt.ylabel('Density', fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(rf'FPR at $\\alpha={alpha}$ FDR', fontsize=14)\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/tpr_fdr.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Protein-vec: Histogram of Thresholds')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "plt.hist(np.array(lhats), color='skyblue')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Protein-vec: Histogram of Thresholds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNR Control Experiments\n",
    "\n",
    "Similar to FDR control but now focusing on False Negative Rate:\n",
    "- Find threshold that controls FNR at desired level (e.g., α=0.1)\n",
    "- Test on independent data to verify control\n",
    "- Also measure what percentage of data gets retrieved (higher FNR control means higher thresholds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'FDR Risk controlled at $\\\\alpha=0.1$')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "weights = np.ones_like(risks)/float(len(risks))\n",
    "plt.hist(np.array(risks), weights=weights, label='Risk', color='skyblue')\n",
    "plt.axvline(alpha, color='black', linestyle='--')\n",
    "plt.xlabel('Risk', fontsize=10)\n",
    "plt.ylabel('Density', fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(rf'FDR Risk controlled at $\\alpha={alpha}$', fontsize=14)\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/fdr_control.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_115/3441051359.py:23: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  lhat = get_thresh_new(X_cal, y_cal, alpha)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:46<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "num_trials = 100\n",
    "alpha = 0.1\n",
    "n_calib = 100\n",
    "fnrs = []\n",
    "retrieved_data_percentages = []\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    # Randomly split data into calibration and test sets\n",
    "    np.random.shuffle(data)\n",
    "    cal_data = data[:n_calib]\n",
    "    test_data = data[n_calib:3*n_calib]\n",
    "\n",
    "    # lhat = get_thresh_new(cal_data, alpha)\n",
    "    # lhat = get_thresh(cal_data, alpha)\n",
    "    # error, fraction_inexact, error_partial, fraction_partial = validate_lhat(test_data, lhat)\n",
    "\n",
    "    # TODO: add lhat to print\n",
    "    # TODO: difference between get_thresh and get_thresh_FDR\n",
    "    # print(f'Trial {trial+1}: lhat: {lhat}, Ex&!Id/Ex: {error:.2f}, In/Id: {fraction_inexact:.2f}, Pa&!Id/Pa: {error_partial:.2f}, Pa&Id/Id: {fraction_partial:.2f}')\n",
    "\n",
    "    X_cal, y_cal = get_sims_labels(cal_data, partial=False)\n",
    "    X_test, y_test_exact = get_sims_labels(test_data, partial=False)\n",
    "    _, y_test_partial = get_sims_labels(test_data, partial=True)\n",
    "    lhat = get_thresh_new(X_cal, y_cal, alpha)\n",
    "    error, fraction_inexact, error_partial, fraction_partial, fpr = validate_lhat_new(X_test, y_test_partial, y_test_exact, lhat)\n",
    "    # print(f'Trial {trial+1}: lhat: {lhat}, Ex&!Id/Ex (FNR): {error:.2f}, In/Id (FDR): {fraction_inexact:.2f}, Pa&!Id/Pa (FNR partial): {error_partial:.2f}, Pa&Id/Id: {fraction_partial:.2f} (TPR partial), FPR: {fpr:.2f}')\n",
    "    fnrs.append(error)\n",
    "    retrieved_data_percentages.append((X_test.flatten() >= lhat).sum() / len(X_test.flatten()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "weights = np.ones_like(retrieved_data_percentages)/float(len(retrieved_data_percentages))\n",
    "plt.hist(np.array(retrieved_data_percentages) * 100, weights=weights, color='skyblue')\n",
    "plt.xlabel('% data retrieved', fontsize=10)\n",
    "plt.ylabel('Density', fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(rf'% data retrieved at $\\alpha={alpha}$ FNR', fontsize=14)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.2f}'))\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/data_retrived.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn-Abers Prediction\n",
    "\n",
    " Instead of just similarity scores, estimate actual probabilities.\n",
    "\n",
    "Venn-Abers gives prediction intervals [p⁰, p¹] for the probability that a protein match is correct.\n",
    "- Wider intervals = less certain\n",
    "- Well-calibrated predictions should have the true probability fall within the interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'FNR controlled at $\\\\alpha=0.1$')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "weights = np.ones_like(fnrs)/float(len(fnrs))\n",
    "plt.hist(np.array(fnrs), weights=weights, color='skyblue')\n",
    "plt.axvline(alpha, color='black', linestyle='--')\n",
    "plt.xlabel('False negative rate', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(rf'FNR controlled at $\\alpha={alpha}$', fontsize=14)\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/fnr_control.pdf', format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venn-Abers prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_calib = 50\n",
    "\n",
    "np.random.shuffle(data)\n",
    "cal_data = data[:n_calib]\n",
    "test_data = data[n_calib:3*n_calib]\n",
    "\n",
    "\n",
    "X_cal, y_cal = get_sims_labels(cal_data, partial=False)\n",
    "X_test, y_test_exact = get_sims_labels(test_data, partial=False)\n",
    "# flatten the data\n",
    "X_cal = X_cal.flatten()\n",
    "y_cal = y_cal.flatten()\n",
    "X_test = X_test.flatten()\n",
    "y_test_exact = y_test_exact.flatten()\n",
    "\n",
    "\n",
    "# np.random.shuffle(data)\n",
    "# X_cal = sims[:n_calib, :].flatten()\n",
    "# y_cal = labels[:n_calib].flatten()\n",
    "# cal_data = data[:n_calib]\n",
    "# X_test = sims[n_calib:2*n_calib, :].flatten()\n",
    "# y_test = labels[n_calib:2*n_calib].flatten()\n",
    "# test_data = data[n_calib:3*n_calib]\n",
    "# ir = get_isotone_regression(cal_data)\n",
    "# sims, labels = get_sims_labels(test_data, partial=False)\n",
    "p_0, p_1 = simplifed_venn_abers_prediction(X_cal, y_cal, X_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.991298, 0.9845901)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_0, p_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [02:58<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_trial(data, n_calib):\n",
    "    np.random.shuffle(data)\n",
    "    cal_data = data[:n_calib]\n",
    "    test_data = data[n_calib:3*n_calib]\n",
    "    X_cal, y_cal = get_sims_labels(cal_data, partial=False)\n",
    "    X_test, y_test_exact = get_sims_labels(test_data, partial=False)\n",
    "    # flatten the data\n",
    "    X_cal = X_cal.flatten()\n",
    "    y_cal = y_cal.flatten()\n",
    "    X_test = X_test.flatten()\n",
    "    y_test_exact = y_test_exact.flatten()\n",
    "\n",
    "    # generate random indices in the test set\n",
    "    i = np.random.randint(0, len(X_test))\n",
    "    # i_s = np.random.randint(0, len(X_test), int(len(X_test) * args.percent_sva_test))\n",
    "\n",
    "    p_0, p_1 = simplifed_venn_abers_prediction(X_cal, y_cal, X_test[i])\n",
    "    result = (p_0, p_1, X_test[i], y_test_exact[i])\n",
    "    return result\n",
    "\n",
    "num_trials = 1000\n",
    "sva_results = []\n",
    "n_calib = 50\n",
    "for trial in tqdm(range(num_trials)):\n",
    "    # print(f'Running trial {i+1} of {args.num_trials}')\n",
    "    sva_results.append(run_trial(data, n_calib))\n",
    "\n",
    "# sva_results = []\n",
    "# # generate random indices in the test set\n",
    "# i_s = np.random.randint(0, len(X_test), 10000)\n",
    "\n",
    "# for _, i in tqdm(enumerate(i_s)):\n",
    "#     p_0, p_1 = simplifed_venn_abers_prediction(X_cal, y_cal, X_test[i])\n",
    "#     sva_results.append((p_0, p_1, y_test_exact[i]))\n",
    "    # print(f'Prediction: {p_1}, Actual: {y_test[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_p = [np.abs(p[0] - p[1]) for p in sva_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0], [Text(0, 0, '')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.5, 2.5))\n",
    "# weights = np.ones_like(abs_p) / len(abs_p)\n",
    "# plt.hist(abs_p, bins=50, color='skyblue', weights=weights)\n",
    "sns.violinplot(x=abs_p, color='skyblue')\n",
    "plt.xlabel(r'$|\\hat{p}^0 - \\hat{p}^1|$', fontsize=10)\n",
    "plt.ylabel('Density', fontsize=10)\n",
    "plt.title('Calibration of Venn-Abers Predictions', fontsize=14)\n",
    "# plt.xticks(fontsize=10)\n",
    "x_ticks = np.linspace(min(abs_p), max(abs_p), num=4)\n",
    "plt.xticks(x_ticks, [f'{tick:.5f}' for tick in x_ticks], fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "# plt.grid(True, linestyle='--', alpha=0.5)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/sva_calibration.pdf', format='pdf')\n",
    "# plt.savefig('/data/ron/protein-conformal/figs/pfam_new/sva_calibration.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Isotonic Regression **\n",
    "\n",
    "Shows how similarity scores map to actual probabilities of correct matches:\n",
    "- X-axis: similarity scores  \n",
    "- Y-axis: predicted probability of match being correct\n",
    "- Histogram shows distribution of similarity scores for true matches (1) vs false matches (0)\n",
    "\n",
    "This helps understand the relationship between similarity and match accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1\n",
    "n_calib = 1000\n",
    "for trial in range(num_trials):\n",
    "    # Randomly split data into calibration and test sets\n",
    "    np.random.shuffle(data)\n",
    "    cal_data = data[:n_calib]\n",
    "    test_data = data[n_calib:2*n_calib]\n",
    "    X_cal, y_cal = get_sims_labels(cal_data, partial=False)\n",
    "    ir = get_isotone_regression(X_cal.flatten(), y_cal.flatten())\n",
    "    X_test, y_test = get_sims_labels(test_data, partial=False)\n",
    "    preds = ir.predict(X_test.flatten())\n",
    "\n",
    "    plt.figure(figsize=(3.5, 2.5))\n",
    "    g = sns.JointGrid(height=5, ratio=5, space=0.2)\n",
    "    # g.ax_joint.scatter(X_test[y_test], y_test[y_test], alpha=0.002)\n",
    "    # g.ax_joint.scatter(X_test[~y_test], y_test[~y_test], alpha=0.002)\n",
    "    x = np.linspace(np.min(X_test), np.max(X_test), 1000)\n",
    "\n",
    "    phats = ir.predict(x)\n",
    "    g.ax_joint.plot(x, phats)\n",
    "    g.set_axis_labels(r'Similarity $S_{ij}$', 'Predicted probability', fontsize=10)\n",
    "\n",
    "    weights_1 = np.ones_like(y_test[y_test]) / len(y_test[y_test])\n",
    "    g.ax_marg_x.hist(X_test[y_test], label='1', alpha=0.9, bins=100, density=True, weights=weights_1)\n",
    "\n",
    "    weights_0 = np.ones_like(y_test[~y_test]) / len(y_test[~y_test])\n",
    "    g.ax_marg_x.hist(X_test[~y_test], label='0', alpha=0.4, bins=100, density=True, weights=weights_0)\n",
    "    # Manually create a new axis for the marginal histogram and set its y-axis label\n",
    "    ax_marg_y = g.figure.add_axes(g.ax_marg_x.get_position())\n",
    "    ax_marg_y.yaxis.tick_right()\n",
    "    # ax_marg_y.set_ylabel('Density', labelpad=15, fontsize=10)\n",
    "    ax_marg_y.set_ylabel('Density', fontsize=10)\n",
    "    ax_marg_y.yaxis.set_label_position('left')\n",
    "    ax_marg_y.set_yticks([])\n",
    "    ax_marg_y.set_frame_on(False)\n",
    "    ax_marg_y.set_xticks([])\n",
    "\n",
    "    # Set fewer x-axis ticks\n",
    "    x_ticks = np.linspace(np.min(X_test), np.max(X_test), 5)\n",
    "    g.ax_joint.set_xticks(x_ticks)\n",
    "    g.ax_joint.set_xticklabels([f'{tick:.5f}' for tick in x_ticks], fontsize=10)\n",
    "\n",
    "    handles, labels = g.ax_marg_x.get_legend_handles_labels()\n",
    "    g.ax_marg_x.legend(handles, labels, title='Class')\n",
    "\n",
    "    # Optionally remove the spines for a cleaner look\n",
    "    # g.ax_marg_x.spines['bottom'].set_visible(False)\n",
    "    # g.ax_marg_x.spines['top'].set_visible(False)\n",
    "    # g.ax_marg_x.spines['left'].set_visible(False)\n",
    "    # g.ax_marg_x.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(r'Predicted Probability for $S_{ij}$', y=1.05, fontsize=14)\n",
    "    # Adjust subplots to fit the figure size\n",
    "    plt.subplots_adjust(left=0.15, right=0.95, top=.95, bottom=0.2)\n",
    "    # plt.savefig('/data/ron/protein-conformal/figs/pfam_new/isotonic_regression.pdf', format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    ##print()\n",
    "    #plt.figure()\n",
    "    #sim_bins = np.linspace(min(sims), max(sims), 100)\n",
    "    #plt.hist(np.array(sims)[np.array(labels)], bins=sim_bins, alpha=0.5, label='1')\n",
    "    #plt.hist(np.array(sims)[~np.array(labels)], bins=sim_bins, alpha=0.5, label='0')\n",
    "    #plt.legend()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
