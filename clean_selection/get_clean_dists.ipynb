{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLEAN.infer import infer_pvalue\n",
    "\n",
    "test_data = \"new\"\n",
    "train_data = \"split100\"\n",
    "infer_pvalue(train_data, test_data, p_value=1e-5, nk_random=20, report_metrics=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similar code to the selection methods, except we just want to extract the raw euclidean distance maps for any pair of train and test data\n",
    "import torch\n",
    "from CLEAN.utils import * \n",
    "from CLEAN.model import LayerNormNet\n",
    "from CLEAN.distance_map import *\n",
    "from CLEAN.evaluate import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def get_eval_dist_map(train_data, test_data, pretrained=True, model_name=None):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    dtype = torch.float32\n",
    "    id_ec_train, ec_id_dict_train = get_ec_id_dict('./data/' + train_data + '.csv')\n",
    "    id_ec_test, _ = get_ec_id_dict('./data/' + test_data + '.csv')\n",
    "    # load checkpoints\n",
    "    # NOTE: change this to LayerNormNet(512, 256, device, dtype) \n",
    "    # and rebuild with [python build.py install]\n",
    "    # if inferencing on model trained with supconH loss\n",
    "    model = LayerNormNet(512, 128, device, dtype)\n",
    "    \n",
    "    if pretrained:\n",
    "        try:\n",
    "            checkpoint = torch.load('./data/pretrained/'+ train_data +'.pth', map_location=device)\n",
    "        except FileNotFoundError as error:\n",
    "            raise Exception('No pretrained weights for this training data')\n",
    "    else:\n",
    "        try:\n",
    "            checkpoint = torch.load('./data/model/'+ model_name +'.pth', map_location=device)\n",
    "        except FileNotFoundError as error:\n",
    "            raise Exception('No model found!')\n",
    "        \n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    # load precomputed EC cluster center embeddings if possible\n",
    "    if train_data == \"split70\":\n",
    "        emb_train = torch.load('./data/pretrained/70.pt', map_location=device)\n",
    "    elif train_data == \"split100\":\n",
    "        emb_train = torch.load('./data/pretrained/100.pt', map_location=device)\n",
    "    else:\n",
    "        emb_train = model(esm_embedding(ec_id_dict_train, device, dtype))\n",
    "        \n",
    "    emb_test = model_embedding_test(id_ec_test, model, device, dtype)\n",
    "    eval_dist = get_dist_map_test(emb_train, emb_test, ec_id_dict_train, id_ec_test, device, dtype)\n",
    "    seed_everything()\n",
    "\n",
    "    return eval_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dist = get_eval_dist_map(train_data, test_data, pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get keys of dict as list\n",
    "test_ids = list(eval_dist.keys())\n",
    "\n",
    "# each key in the dictionary is a dictionary, and we want to sort the sub-dictionary based on the values of the keys ascending\n",
    "sorted_dict = {key: dict(sorted(eval_dist[key].items(), key=lambda item: item[1])) for key in eval_dist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create two numpy arrays, one for the keys and one for the values, of size (len(test_ids), # of keys in the sub-dictionary)\n",
    "## Go through each key in sorted_dict,\n",
    "## For each key, go through each key in the sub-dictionary and get the key value pair\n",
    "## Append the key to the keys array and the value to the values array\n",
    "import numpy as np\n",
    "\n",
    "## init 2d np arrays with 0's\n",
    "\n",
    "## np array of EC_ids (strings)\n",
    "#EC_ids = np.zeros((len(test_ids), len(sorted_dict[test_ids[0]]))\n",
    "dists = np.zeros((len(test_ids), len(sorted_dict[test_ids[0]])))\n",
    "for i, key in enumerate(sorted_dict):\n",
    "    j = 0\n",
    "    for k, v in sorted_dict[key].items():\n",
    "        dists[i][j] = v\n",
    "        j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# find first index where element dists[0] is not ascedning\n",
    "def find_non_ascending_row(arr):\n",
    "    for i in range(arr.shape[0]):\n",
    "        if not np.all(np.diff(arr[i]) >= 0):\n",
    "            return i\n",
    "    return -1  # Return -1 if all rows are ascending\n",
    "\n",
    "# Example usage\n",
    "non_ascending_row_index = find_non_ascending_row(dists)\n",
    "print(non_ascending_row_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sorted_dict\n",
    "import pickle\n",
    "with open('/home/seyonec/protein-conformal/clean_selection/sorted_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(sorted_dict, f)\n",
    "\n",
    "# save dists\n",
    "with open('/home/seyonec/protein-conformal/clean_selection/dists.pkl', 'wb') as f:\n",
    "    pickle.dump(dists, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
